{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import build_corpus, build_map\n",
    "from codecs import open\n",
    "from os.path import join\n",
    "from evaluate import ensemble_evaluate\n",
    "from evaluating import Metrics\n",
    "from utils import load_model, extend_maps, prepocess_data_for_lstmcrf\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\")  # 将上级目录添加到Python路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户输入句子\n",
    "inputStr = input(\"请输入句子：\")\n",
    "input_text = inputStr.strip() # 去掉首尾空格\n",
    "output_file = \"./ResumeNER/predict.char.bmes\"\n",
    "def text_to_bmes(input_text, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for char in input_text:\n",
    "            if char == '\\n':\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write(f'{char} O\\n')\n",
    "        f.write('\\n')  #保证bmes文件里面的空行\n",
    "text_to_bmes(input_text, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(split, make_vocab=True, data_dir=\"./ResumeNER\"):\n",
    "    \"\"\"读取数据\"\"\"\n",
    "    print(f\"Provided split value: {split}\")  # 添加输出以查看传入的split值\n",
    "    assert split in ['train', 'dev', 'test','predict']\n",
    "\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(join(data_dir, split+\".char.bmes\"), 'r', encoding='utf-8') as f:\n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        for line in f:\n",
    "            # if line != '\\r\\n':\n",
    "            #     word, tag = line.strip('\\n').split()\n",
    "            #     word_list.append(word)\n",
    "            #     tag_list.append(tag)\n",
    "            if line.strip():  # 如果不是空行\n",
    "                word, tag = line.strip().split()\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "            else:\n",
    "                word_lists.append(word_list)\n",
    "                tag_lists.append(tag_list)\n",
    "                word_list = []\n",
    "                tag_list = []\n",
    "\n",
    "    # 如果make_vocab为True，还需要返回word2id和tag2id\n",
    "    if make_vocab:\n",
    "        word2id = build_map(word_lists)\n",
    "        tag2id = build_map(tag_lists)\n",
    "        return word_lists, tag_lists, word2id, tag2id\n",
    "    else:\n",
    "        return word_lists, tag_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    predict_word_lists, predict_tag_lists = build_corpus(\"predict\", make_vocab=False,data_dir=\"./ResumeNER\")\n",
    "    train_word_lists, train_tag_lists, word2id, tag2id = build_corpus(\"train\",data_dir=\"./ResumeNER\")\n",
    "    crf_word2id, crf_tag2id = extend_maps(word2id, tag2id, for_crf=True)\n",
    "    bilstm_model = load_model('./ckpts/bilstm_crf.pkl')\n",
    "    bilstm_model.model.bilstm.bilstm.flatten_parameters()  # remove warning\n",
    "    predict_word_lists, predict_tag_lists = prepocess_data_for_lstmcrf(\n",
    "        predict_word_lists, predict_tag_lists, test=True\n",
    "    )\n",
    "    lstmcrf_pred, target_tag_list = bilstm_model.test(predict_word_lists, predict_tag_lists,\n",
    "                                                      crf_word2id, crf_tag2id)\n",
    "\n",
    "    # print(lstmcrf_pred)\n",
    "    return lstmcrf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided split value: predict\n",
      "Provided split value: train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'M-ORG',\n",
       "  'E-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'O',\n",
       "  'M-ORG',\n",
       "  'M-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'M-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided split value: predict\n",
      "Provided split value: train\n",
      "{'': '法院'}\n"
     ]
    }
   ],
   "source": [
    "text = input_text\n",
    "tags = predict()[0]\n",
    "# 定义一个函数，将标签列表转换为实体字典\n",
    "def tags_to_dict(text, tags):\n",
    "    entities = {}\n",
    "    entity = ''\n",
    "    entity_type = ''\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.startswith('B-'):\n",
    "            # 如果是实体的开始，将当前实体清空，并开始一个新的实体\n",
    "            entity = text[i]\n",
    "            entity_type = tag[2:]  # 获取实体类型\n",
    "        elif tag.startswith('M-'):\n",
    "            # 如果是实体的中间部分，将当前字加入实体\n",
    "            entity += text[i]\n",
    "        elif tag.startswith('E-'):\n",
    "            # 如果是实体的结束，将当前字加入实体，并将实体加入实体字典\n",
    "            entity += text[i]\n",
    "            entities[entity_type] = entity\n",
    "            entity = ''\n",
    "            entity_type = ''\n",
    "        else:\n",
    "            # 如果不是实体，将当前实体清空\n",
    "            entity = ''\n",
    "            entity_type = ''\n",
    "    return entities\n",
    "\n",
    "# 调用函数，将标签列表转换为实体字典\n",
    "entities = tags_to_dict(text, tags)\n",
    "\n",
    "# 输出实体字典\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': '法院', '日期': '2011年10月28日'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_dates(text):\n",
    "    # 正则表达式，用于识别常见的日期格式\n",
    "    date_pattern = r\"\"\"(?P<date>\n",
    "        (?P<year_only>\\d{4}(?![\\d年]))|                                       # 单独的年份，如 2012，2017\n",
    "        # YYYY-MM-DD 或 YYYY/MM/DD 或 YYYY.MM.DD 或 YYYY年MM月DD日\n",
    "        (?P<year>\\d{4})[-/年.](?P<month>\\d{1,2})[-/月.]?(?P<day>\\d{1,2})?[日]?|\n",
    "        # DD-MM-YYYY 或 DD/MM/YYYY 或 DD.MM.YYYY\n",
    "        (?P<day2>\\d{1,2})[-/.](?P<month2>\\d{1,2})[-/.](?P<year2>\\d{4})|\n",
    "        # MM-DD-YYYY 或 MM/DD/YYYY 或 MM.DD.YYYY\n",
    "        (?P<month3>\\d{1,2})[-/.](?P<day3>\\d{1,2})[-/.](?P<year3>\\d{2,4})\n",
    "    )\"\"\"\n",
    "\n",
    "    # 使用正则表达式查找日期\n",
    "\n",
    "    matches = re.finditer(date_pattern, text, re.VERBOSE)\n",
    "    result = []\n",
    "\n",
    "    # 将匹配的日期添加到结果列表中\n",
    "    for match in matches:\n",
    "\n",
    "        date_str = match.group('date')\n",
    "        # append的内容是 \"日期\":\"date_str\"，如\"日期\":\"2010年3月3日\"\n",
    "        result.append({\"日期\": date_str})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# 示例\n",
    "textDate = input_text\n",
    "# entities = entities + extract_dates(textDate)\n",
    "for item in extract_dates(textDate):\n",
    "    entities.update(item)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': '法院', '日期': '2011年10月28日'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class CaseNumberRecognizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = r\"[（(]\\d{4}[）)]\\S+\\d+号|第\\d+号\"\n",
    "\n",
    "    def recognize(self, text):\n",
    "        results = re.findall(self.pattern, text)\n",
    "        if results:\n",
    "            return [{\"案号\":result} for result in results]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "text = input_text\n",
    "recognizer = CaseNumberRecognizer()\n",
    "results = recognizer.recognize(text)\n",
    "# 如果返回不为空，将结果添加到实体字典中\n",
    "if results:\n",
    "    for res in results:\n",
    "        entities.update(res)\n",
    "\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ahoy ye landlubbers, listen to me tale of the ChatGPT API! Avast ye, it's the most remarkable treasure ye ever did see, with the power to make ye messaging dreams come true. With just a few clicks, ye can integrate our API into ye application or website and be on ye way to smoother messaging and conversations!\n",
      "\n",
      "Our API is so easy to use, even a scallywag can do it. It's designed to be user-friendly, with a simple interface that even the most inexperienced swashbuckler can navigate with ease. Our ChatGPT API is a real boon to ye and ye crew, as it brings new features like smart message prediction, multi-language support, and sentiment analysis, making ye messaging experience smoother and more delightful than ever before.\n",
      "\n",
      "The ChatGPT API is truly one of a kind, with a robust data backend that can handle any conversation ye throw at it. Be it for customer service, support, or just regular chats, our API can handle it all! So, ye landlubbers, don't be shy about testing out our ChatGPT API - it's the answer to all ye messaging woes!\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Tell the world about the ChatGPT API in the style of a pirate.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
